# ğŸš€ Redis Cache Service (Async Python Demo)

A **modern async caching service** built on **Redis** to demonstrate:

- ğŸ” Read/Write-through caching
- ğŸ“¡ Event-driven invalidation using Redis **Pub/Sub**
- ğŸ§© Concurrency-safe async design with **asyncio**
- ğŸ§° Command-line interface powered by **Typer**
- âœ… Full test coverage with **pytest + fakeredis**

## ğŸ§± Quick Start

```bash
# Install dependencies
uv sync

# Explore CLI options
uv run src\cli.py --help
```

## ğŸ“‚ Project Structure

```
redis-cache-service-py/
â”‚
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py                    # CLI entrypoint (Typer)
â”‚   â”œâ”€â”€ async_cli.py              # Async CLI variant
â”‚   â”œâ”€â”€ redis_utils.py            # Redis helper functions
â”‚   â””â”€â”€ cache/
â”‚       â”œâ”€â”€ manager.py            # Core cache read/write logic
â”‚       â”œâ”€â”€ lock_table.py         # Per-key asyncio locks (anti-stampede)
â”‚       â””â”€â”€ stampede_prevention_demo.py  # Demo script
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_cache_manager.py     # Integration & stampede prevention tests
    â”œâ”€â”€ test_cli.py               # CLI command tests
    â”œâ”€â”€ test_redis_utils.py       # Redis utility tests
    â””â”€â”€ conftest.py               # Pytest fixtures
```

## Cache Stampede Prevention Demo

This project demonstrates **cache stampede mitigation** â€” preventing multiple concurrent clients from redundantly fetching the same missing key.

### ğŸ” Problem

When a popular cache key expires or is invalidated, many concurrent requests may try to reload it at once. Without coordination, this can overwhelm the backend or data source.

### ğŸ’¡ Solution

The service uses **per-key async locks** (`LockTable`) to ensure:

- The first requester acquires the lock and triggers the backend fetch.
- All other concurrent requests for the same key await the same lock.
- Once the result is fetched and cached, all waiting requests are served from cache immediately.

This eliminates duplicate backend fetches and prevents stampedes.

### âš™ï¸ How It Works

Each key has a unique `asyncio.Lock` managed by a lightweight in-memory **LockTable**:

```python
# Simplified logic
async def get_or_set(key, loader):
    async with lock_table[key]:         # Only one fetch per key
        if key not in cache:
            cache[key] = await loader() # Load once
    return cache[key]
```

### ğŸš€ Run the Demo

```bash
uv run -m src.cache.stampede_prevention_demo
```

**Sample Output:**

```
All results: [22, 22, 22, 22, 22]
```

All concurrent requests share a **single backend call**, proving effective stampede prevention.

---

## ğŸ§° Tech Stack

| Component                    | Description                    |
| ---------------------------- | ------------------------------ |
| **Python 3.11+**             | Core language                  |
| **Redis (Upstash or local)** | Backend cache                  |
| **asyncio**                  | Async concurrency              |
| **Typer**                    | Modern CLI framework           |
| **pytest + fakeredis**       | Testing & mocking              |
| **uv**                       | Lightweight dependency manager |

## ğŸ§ª Tests in Action

Run all tests:

```bash
uv run pytest -v
```

Example focus test for stampede prevention:

```bash
uv run pytest -k test_simple_cache_coalesces_requests -v
```

All tests simulate **concurrent clients, lock behavior, and cache invalidation**.

Pending, screen capture to be added

## ğŸ”® Future Enhancements

- ğŸ”§ Extensible cache strategy architecture
- ğŸ” Distributed lock mechanism using Redis for multi-instance setups
- ğŸ“Š Real-time metrics (hit/miss rates, contention) via Grafana or CLI dashboards
- ğŸ§© Pluggable invalidation policies and event hooks

### â­ If you found this project interesting â€” star it on GitHub!
